{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b319da-a4d2-4ceb-90ed-66ad5192c2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 01:33:03.063652: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-29 01:33:04.489707: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-29 01:33:08.246857: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-29 01:33:08.507548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 01:33:41.746216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2acaed3-0d87-4197-a0f1-6f665357e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shape_info(dir):\n",
    "    import pickle\n",
    "    with open(dir + '/pickleshapes', 'rb') as file:\n",
    "        # Load the data from the file\n",
    "        shapes = pickle.load(file)\n",
    "        return shapes\n",
    "def _lab_labels(all_labels):\n",
    "        #, this is an embedded function called from below\n",
    "        labels = {}\n",
    "        labels['comp_labels'] = tf.one_hot(tf.cast(all_labels[0],tf.int32),3)\n",
    "        labels['amp_labels'] = tf.one_hot(tf.cast(all_labels[1],tf.int32),10)\n",
    "        labels['torque_labels'] = tf.one_hot(tf.cast(all_labels[2],tf.int32),5)\n",
    "        labels['joint_labels'] = tf.one_hot(tf.cast(all_labels[3],tf.int32),3)\n",
    "        return labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa445be-82b6-40da-8159-7e4cbe8fe209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tf_data(folder,shape_info):\n",
    "    tf_records = glob.glob(os.path.join(folder, '*.tf'))\n",
    "    dataset = tf.data.TFRecordDataset(tf_records)\n",
    "    def _parse_function(example_proto):\n",
    "        feature_description = {\n",
    "            'ts': tf.io.FixedLenFeature(shape_info[0], tf.float32),\n",
    "            'labels': tf.io.FixedLenFeature(shape_info[1], tf.float32),\n",
    "            'pos_labels': tf.io.FixedLenFeature(shape_info[2], tf.float32)\n",
    "        }\n",
    "        #example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        example = tf.io.parse_example(example_proto, feature_description)\n",
    "        all_labels = tf.cast(example['labels'], tf.float32)\n",
    "        #all_labels = example['labels']\n",
    "        labels = _lab_labels(all_labels)\n",
    "        if 'pos_labels' in example.keys():\n",
    "            pos_labels = tf.cast(example['pos_labels'], tf.float32)\n",
    "            labels['pos_labels'] = pos_labels\n",
    "        ts = example['ts']\n",
    "        #labels = all_labels\n",
    "        return ts, labels['comp_labels']\n",
    "        #return example['ts'], labels\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    dataset = dataset.batch(batch_size=500, drop_remainder = False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8676d07f-20ce-46b9-8b6f-18be371473f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_base = '/scratch/user/swati/Capstone/'\n",
    "#dir_data = 'RoughCut_Datasets' /\n",
    "dir_base = '/scratch/group/statconsult/Output_hema/'\n",
    "dir_data = 'Data6_Copy/'\n",
    "dir = dir_base + dir_data + '/'\n",
    "train_shape_info = load_shape_info(dir + 'train')\n",
    "test_shape_info = load_shape_info(dir + 'predict')\n",
    "predict_shape_info = load_shape_info(dir + 'validate')\n",
    "train_data = load_tf_data(dir + 'train',train_shape_info)\n",
    "test_data = load_tf_data(dir + 'predict',test_shape_info)\n",
    "predict_data = load_tf_data(dir + 'validate',predict_shape_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8153a569-b766-4327-8bae-0a1afd16b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 105000)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               13440128  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13448579 (51.30 MB)\n",
      "Trainable params: 13448579 (51.30 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# # Define the model\n",
    "# model = keras.Sequential()\n",
    "\n",
    "# # Flatten the input data\n",
    "# model.add(keras.layers.Flatten(input_shape=(1000, 50)))\n",
    "\n",
    "# # Add a couple of dense layers\n",
    "# model.add(keras.layers.Dense(128, activation='relu'))\n",
    "# model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "# # Output layer with 4 units (assuming you have 4 classes for classification)\n",
    "# model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Flatten(input_shape=(1000, 50)))\n",
    "# model.add(keras.layers.Dense(128, activation='relu'))\n",
    "# model.add(keras.layers.Dropout(0.5))  # Adding dropout for regularization\n",
    "# model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# model.add(keras.layers.Dropout(0.5))\n",
    "# model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# # Compile the model with a lower learning rate\n",
    "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "#model.summary()\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(1000, 105)))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))  # Adding dropout for regularization\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#     layers.Flatten(input_shape=(1000, 50)),\n",
    "#     layers.BatchNormalization(),\n",
    "    \n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "\n",
    "#     layers.Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Use a different optimizer (SGD) and add learning rate scheduling\n",
    "# opt = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c718f1a7-3f27-47f9-afc3-95c58d8ad2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "216/216 [==============================] - 36s 158ms/step - loss: 1.1472 - accuracy: 0.3369 - val_loss: 1.0135 - val_accuracy: 0.4167\n",
      "Epoch 2/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.9083 - accuracy: 0.5474 - val_loss: 0.8821 - val_accuracy: 0.5273\n",
      "Epoch 3/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.4749 - accuracy: 0.8276 - val_loss: 0.9421 - val_accuracy: 0.7595\n",
      "Epoch 4/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.1912 - accuracy: 0.9486 - val_loss: 1.1538 - val_accuracy: 0.8233\n",
      "Epoch 5/100\n",
      "216/216 [==============================] - 24s 112ms/step - loss: 0.1027 - accuracy: 0.9740 - val_loss: 1.2867 - val_accuracy: 0.8527\n",
      "Epoch 6/100\n",
      "216/216 [==============================] - 25s 116ms/step - loss: 0.0666 - accuracy: 0.9834 - val_loss: 1.4073 - val_accuracy: 0.8603\n",
      "Epoch 7/100\n",
      "216/216 [==============================] - 25s 118ms/step - loss: 0.0496 - accuracy: 0.9867 - val_loss: 1.5214 - val_accuracy: 0.8615\n",
      "Epoch 8/100\n",
      "216/216 [==============================] - 24s 112ms/step - loss: 0.0406 - accuracy: 0.9896 - val_loss: 1.6100 - val_accuracy: 0.8633\n",
      "Epoch 9/100\n",
      "216/216 [==============================] - 26s 120ms/step - loss: 0.0364 - accuracy: 0.9900 - val_loss: 1.6708 - val_accuracy: 0.8670\n",
      "Epoch 10/100\n",
      "216/216 [==============================] - 26s 119ms/step - loss: 0.0310 - accuracy: 0.9918 - val_loss: 1.7358 - val_accuracy: 0.8680\n",
      "Epoch 11/100\n",
      "216/216 [==============================] - 26s 119ms/step - loss: 0.0301 - accuracy: 0.9920 - val_loss: 1.7708 - val_accuracy: 0.8718\n",
      "Epoch 12/100\n",
      "216/216 [==============================] - 26s 119ms/step - loss: 0.0301 - accuracy: 0.9919 - val_loss: 1.7660 - val_accuracy: 0.8738\n",
      "Epoch 13/100\n",
      "216/216 [==============================] - 27s 126ms/step - loss: 0.0257 - accuracy: 0.9932 - val_loss: 1.7838 - val_accuracy: 0.8745\n",
      "Epoch 14/100\n",
      "216/216 [==============================] - 27s 125ms/step - loss: 0.0233 - accuracy: 0.9938 - val_loss: 1.8703 - val_accuracy: 0.8790\n",
      "Epoch 15/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.0217 - accuracy: 0.9945 - val_loss: 1.9110 - val_accuracy: 0.8783\n",
      "Epoch 16/100\n",
      "216/216 [==============================] - 26s 120ms/step - loss: 0.0206 - accuracy: 0.9948 - val_loss: 1.9540 - val_accuracy: 0.8780\n",
      "Epoch 17/100\n",
      "216/216 [==============================] - 27s 123ms/step - loss: 0.0189 - accuracy: 0.9949 - val_loss: 1.9732 - val_accuracy: 0.8765\n",
      "Epoch 18/100\n",
      "216/216 [==============================] - 27s 125ms/step - loss: 0.0205 - accuracy: 0.9949 - val_loss: 1.9820 - val_accuracy: 0.8768\n",
      "Epoch 19/100\n",
      "216/216 [==============================] - 26s 122ms/step - loss: 0.0176 - accuracy: 0.9956 - val_loss: 1.9958 - val_accuracy: 0.8728\n",
      "Epoch 20/100\n",
      "216/216 [==============================] - 27s 123ms/step - loss: 0.0188 - accuracy: 0.9951 - val_loss: 2.0397 - val_accuracy: 0.8738\n",
      "Epoch 21/100\n",
      "216/216 [==============================] - 26s 120ms/step - loss: 0.0190 - accuracy: 0.9952 - val_loss: 2.0235 - val_accuracy: 0.8772\n",
      "Epoch 22/100\n",
      "216/216 [==============================] - 26s 120ms/step - loss: 0.0176 - accuracy: 0.9956 - val_loss: 2.0589 - val_accuracy: 0.8740\n",
      "Epoch 23/100\n",
      "216/216 [==============================] - 25s 118ms/step - loss: 0.0171 - accuracy: 0.9956 - val_loss: 2.0727 - val_accuracy: 0.8762\n",
      "Epoch 24/100\n",
      "216/216 [==============================] - 26s 122ms/step - loss: 0.0164 - accuracy: 0.9960 - val_loss: 2.1291 - val_accuracy: 0.8758\n",
      "Epoch 25/100\n",
      "216/216 [==============================] - 26s 122ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 2.1429 - val_accuracy: 0.8753\n",
      "Epoch 26/100\n",
      "216/216 [==============================] - 27s 126ms/step - loss: 0.0169 - accuracy: 0.9958 - val_loss: 2.1560 - val_accuracy: 0.8737\n",
      "Epoch 27/100\n",
      "216/216 [==============================] - 26s 122ms/step - loss: 0.0174 - accuracy: 0.9957 - val_loss: 2.0999 - val_accuracy: 0.8777\n",
      "Epoch 28/100\n",
      "216/216 [==============================] - 27s 123ms/step - loss: 0.0174 - accuracy: 0.9960 - val_loss: 2.1149 - val_accuracy: 0.8773\n",
      "Epoch 29/100\n",
      "216/216 [==============================] - 26s 121ms/step - loss: 0.0153 - accuracy: 0.9961 - val_loss: 2.1476 - val_accuracy: 0.8780\n",
      "Epoch 30/100\n",
      "216/216 [==============================] - 26s 118ms/step - loss: 0.0161 - accuracy: 0.9961 - val_loss: 2.1648 - val_accuracy: 0.8782\n",
      "Epoch 31/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.0168 - accuracy: 0.9957 - val_loss: 2.1749 - val_accuracy: 0.8798\n",
      "Epoch 32/100\n",
      "216/216 [==============================] - 26s 118ms/step - loss: 0.0181 - accuracy: 0.9955 - val_loss: 2.1223 - val_accuracy: 0.8770\n",
      "Epoch 33/100\n",
      "216/216 [==============================] - 27s 125ms/step - loss: 0.0184 - accuracy: 0.9957 - val_loss: 2.0849 - val_accuracy: 0.8805\n",
      "Epoch 34/100\n",
      "216/216 [==============================] - 27s 125ms/step - loss: 0.0177 - accuracy: 0.9957 - val_loss: 2.0986 - val_accuracy: 0.8767\n",
      "Epoch 35/100\n",
      "216/216 [==============================] - 27s 123ms/step - loss: 0.0136 - accuracy: 0.9967 - val_loss: 2.1884 - val_accuracy: 0.8785\n",
      "Epoch 36/100\n",
      "216/216 [==============================] - 26s 121ms/step - loss: 0.0146 - accuracy: 0.9964 - val_loss: 2.1781 - val_accuracy: 0.8813\n",
      "Epoch 37/100\n",
      "216/216 [==============================] - 26s 122ms/step - loss: 0.0147 - accuracy: 0.9965 - val_loss: 2.2251 - val_accuracy: 0.8782\n",
      "Epoch 38/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.0153 - accuracy: 0.9964 - val_loss: 2.1460 - val_accuracy: 0.8807\n",
      "Epoch 39/100\n",
      "216/216 [==============================] - 26s 122ms/step - loss: 0.0144 - accuracy: 0.9964 - val_loss: 2.2215 - val_accuracy: 0.8810\n",
      "Epoch 40/100\n",
      "216/216 [==============================] - 26s 120ms/step - loss: 0.0145 - accuracy: 0.9967 - val_loss: 2.1156 - val_accuracy: 0.8768\n",
      "Epoch 41/100\n",
      "216/216 [==============================] - 26s 121ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 2.1561 - val_accuracy: 0.8758\n",
      "Epoch 42/100\n",
      "216/216 [==============================] - 26s 120ms/step - loss: 0.0145 - accuracy: 0.9965 - val_loss: 2.1577 - val_accuracy: 0.8740\n",
      "Epoch 43/100\n",
      "216/216 [==============================] - 28s 130ms/step - loss: 0.0139 - accuracy: 0.9966 - val_loss: 2.1666 - val_accuracy: 0.8808\n",
      "Epoch 44/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.0150 - accuracy: 0.9964 - val_loss: 2.2330 - val_accuracy: 0.8778\n",
      "Epoch 45/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.0145 - accuracy: 0.9964 - val_loss: 2.1534 - val_accuracy: 0.8777\n",
      "Epoch 46/100\n",
      "216/216 [==============================] - 29s 134ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 2.3121 - val_accuracy: 0.8795\n",
      "Epoch 47/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 2.3275 - val_accuracy: 0.8787\n",
      "Epoch 48/100\n",
      "216/216 [==============================] - 28s 129ms/step - loss: 0.0161 - accuracy: 0.9966 - val_loss: 2.2043 - val_accuracy: 0.8800\n",
      "Epoch 49/100\n",
      "216/216 [==============================] - 28s 129ms/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 2.2293 - val_accuracy: 0.8822\n",
      "Epoch 50/100\n",
      "216/216 [==============================] - 27s 127ms/step - loss: 0.0135 - accuracy: 0.9968 - val_loss: 2.2795 - val_accuracy: 0.8785\n",
      "Epoch 51/100\n",
      "216/216 [==============================] - 27s 124ms/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 2.3138 - val_accuracy: 0.8758\n",
      "Epoch 52/100\n",
      "216/216 [==============================] - 28s 128ms/step - loss: 0.0143 - accuracy: 0.9965 - val_loss: 2.3204 - val_accuracy: 0.8792\n",
      "Epoch 53/100\n",
      "216/216 [==============================] - 29s 132ms/step - loss: 0.0134 - accuracy: 0.9968 - val_loss: 2.3464 - val_accuracy: 0.8823\n",
      "Epoch 54/100\n",
      "216/216 [==============================] - 28s 129ms/step - loss: 0.0132 - accuracy: 0.9970 - val_loss: 2.2675 - val_accuracy: 0.8803\n",
      "Epoch 55/100\n",
      "216/216 [==============================] - 27s 123ms/step - loss: 0.0127 - accuracy: 0.9971 - val_loss: 2.2876 - val_accuracy: 0.8837\n",
      "Epoch 56/100\n",
      "216/216 [==============================] - 26s 119ms/step - loss: 0.0125 - accuracy: 0.9971 - val_loss: 2.3403 - val_accuracy: 0.8837\n",
      "Epoch 57/100\n",
      "216/216 [==============================] - 27s 126ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 2.3321 - val_accuracy: 0.8777\n",
      "Epoch 58/100\n",
      "216/216 [==============================] - 26s 121ms/step - loss: 0.0121 - accuracy: 0.9972 - val_loss: 2.3403 - val_accuracy: 0.8828\n",
      "Epoch 59/100\n",
      "216/216 [==============================] - 31s 142ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 2.3531 - val_accuracy: 0.8778\n",
      "Epoch 60/100\n",
      "216/216 [==============================] - 30s 136ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 2.3508 - val_accuracy: 0.8790\n",
      "Epoch 61/100\n",
      "216/216 [==============================] - 29s 134ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 2.2846 - val_accuracy: 0.8810\n",
      "Epoch 62/100\n",
      "216/216 [==============================] - 30s 139ms/step - loss: 0.0127 - accuracy: 0.9973 - val_loss: 2.3391 - val_accuracy: 0.8775\n",
      "Epoch 63/100\n",
      "216/216 [==============================] - 30s 138ms/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 2.3377 - val_accuracy: 0.8777\n",
      "Epoch 64/100\n",
      "216/216 [==============================] - 30s 139ms/step - loss: 0.0110 - accuracy: 0.9973 - val_loss: 2.3819 - val_accuracy: 0.8770\n",
      "Epoch 65/100\n",
      "216/216 [==============================] - 29s 136ms/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 2.4065 - val_accuracy: 0.8798\n",
      "Epoch 66/100\n",
      "216/216 [==============================] - 31s 142ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 2.4532 - val_accuracy: 0.8792\n",
      "Epoch 67/100\n",
      "216/216 [==============================] - 30s 140ms/step - loss: 0.0111 - accuracy: 0.9976 - val_loss: 2.3298 - val_accuracy: 0.8802\n",
      "Epoch 68/100\n",
      "216/216 [==============================] - 29s 133ms/step - loss: 0.0122 - accuracy: 0.9973 - val_loss: 2.3326 - val_accuracy: 0.8808\n",
      "Epoch 69/100\n",
      "216/216 [==============================] - 30s 140ms/step - loss: 0.0115 - accuracy: 0.9974 - val_loss: 2.3098 - val_accuracy: 0.8797\n",
      "Epoch 70/100\n",
      "216/216 [==============================] - 30s 139ms/step - loss: 0.0110 - accuracy: 0.9974 - val_loss: 2.3391 - val_accuracy: 0.8800\n",
      "Epoch 71/100\n",
      "216/216 [==============================] - 30s 141ms/step - loss: 0.0123 - accuracy: 0.9974 - val_loss: 2.3655 - val_accuracy: 0.8863\n",
      "Epoch 72/100\n",
      "216/216 [==============================] - 31s 142ms/step - loss: 0.0117 - accuracy: 0.9972 - val_loss: 2.3560 - val_accuracy: 0.8820\n",
      "Epoch 73/100\n",
      "216/216 [==============================] - 31s 144ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 2.4034 - val_accuracy: 0.8825\n",
      "Epoch 74/100\n",
      "216/216 [==============================] - 31s 142ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 2.3875 - val_accuracy: 0.8847\n",
      "Epoch 75/100\n",
      "216/216 [==============================] - 31s 141ms/step - loss: 0.0094 - accuracy: 0.9977 - val_loss: 2.4296 - val_accuracy: 0.8865\n",
      "Epoch 76/100\n",
      "216/216 [==============================] - 31s 143ms/step - loss: 0.0093 - accuracy: 0.9977 - val_loss: 2.4378 - val_accuracy: 0.8863\n",
      "Epoch 77/100\n",
      "216/216 [==============================] - 58s 271ms/step - loss: 0.0115 - accuracy: 0.9973 - val_loss: 2.4633 - val_accuracy: 0.8855\n",
      "Epoch 78/100\n",
      "216/216 [==============================] - 31s 141ms/step - loss: 0.0113 - accuracy: 0.9974 - val_loss: 2.4450 - val_accuracy: 0.8832\n",
      "Epoch 79/100\n",
      "216/216 [==============================] - 30s 140ms/step - loss: 0.0092 - accuracy: 0.9978 - val_loss: 2.4622 - val_accuracy: 0.8848\n",
      "Epoch 80/100\n",
      "216/216 [==============================] - 29s 133ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 2.4978 - val_accuracy: 0.8808\n",
      "Epoch 81/100\n",
      "216/216 [==============================] - 29s 134ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 2.4706 - val_accuracy: 0.8803\n",
      "Epoch 82/100\n",
      "216/216 [==============================] - 30s 139ms/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 2.4730 - val_accuracy: 0.8765\n",
      "Epoch 83/100\n",
      "216/216 [==============================] - 28s 131ms/step - loss: 0.0115 - accuracy: 0.9974 - val_loss: 2.4235 - val_accuracy: 0.8773\n",
      "Epoch 84/100\n",
      "216/216 [==============================] - 28s 130ms/step - loss: 0.0105 - accuracy: 0.9977 - val_loss: 2.3277 - val_accuracy: 0.8728\n",
      "Epoch 85/100\n",
      "216/216 [==============================] - 28s 130ms/step - loss: 0.0103 - accuracy: 0.9974 - val_loss: 2.3760 - val_accuracy: 0.8758\n",
      "Epoch 86/100\n",
      "216/216 [==============================] - 28s 132ms/step - loss: 0.0088 - accuracy: 0.9978 - val_loss: 2.4440 - val_accuracy: 0.8808\n",
      "Epoch 87/100\n",
      "216/216 [==============================] - 28s 130ms/step - loss: 0.0108 - accuracy: 0.9978 - val_loss: 2.4046 - val_accuracy: 0.8812\n",
      "Epoch 88/100\n",
      "216/216 [==============================] - 28s 127ms/step - loss: 0.0109 - accuracy: 0.9975 - val_loss: 2.3849 - val_accuracy: 0.8813\n",
      "Epoch 89/100\n",
      "216/216 [==============================] - 28s 130ms/step - loss: 0.0089 - accuracy: 0.9981 - val_loss: 2.4801 - val_accuracy: 0.8807\n",
      "Epoch 90/100\n",
      "216/216 [==============================] - 28s 128ms/step - loss: 0.0093 - accuracy: 0.9978 - val_loss: 2.4910 - val_accuracy: 0.8765\n",
      "Epoch 91/100\n",
      "216/216 [==============================] - 28s 128ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 2.4238 - val_accuracy: 0.8778\n",
      "Epoch 92/100\n",
      "216/216 [==============================] - 28s 130ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 2.4425 - val_accuracy: 0.8803\n",
      "Epoch 93/100\n",
      "216/216 [==============================] - 28s 127ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 2.4940 - val_accuracy: 0.8793\n",
      "Epoch 94/100\n",
      "216/216 [==============================] - 28s 130ms/step - loss: 0.0083 - accuracy: 0.9978 - val_loss: 2.4881 - val_accuracy: 0.8803\n",
      "Epoch 95/100\n",
      "216/216 [==============================] - 29s 135ms/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 2.4988 - val_accuracy: 0.8795\n",
      "Epoch 96/100\n",
      "216/216 [==============================] - 32s 148ms/step - loss: 0.0083 - accuracy: 0.9979 - val_loss: 2.5719 - val_accuracy: 0.8817\n",
      "Epoch 97/100\n",
      "216/216 [==============================] - 32s 148ms/step - loss: 0.0105 - accuracy: 0.9977 - val_loss: 2.3884 - val_accuracy: 0.8788\n",
      "Epoch 98/100\n",
      "216/216 [==============================] - 31s 145ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 2.5198 - val_accuracy: 0.8792\n",
      "Epoch 99/100\n",
      "216/216 [==============================] - 32s 146ms/step - loss: 0.0087 - accuracy: 0.9977 - val_loss: 2.5496 - val_accuracy: 0.8775\n",
      "Epoch 100/100\n",
      "216/216 [==============================] - 32s 147ms/step - loss: 0.0092 - accuracy: 0.9978 - val_loss: 2.5047 - val_accuracy: 0.8772\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the model\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)\n",
    "\n",
    "#history = model1.fit(train_reduced_dataset, callbacks=[lr_scheduler],epochs=num_epochs, validation_data=val_reduced_dataset)\n",
    "history = model.fit(train_data,epochs=num_epochs, validation_data=predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34331733-29ce-47f4-ba28-9a6e1294e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 6s 472ms/step - loss: 0.2418 - accuracy: 0.9422\n",
      "Test Accuracy: 94.22%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on your test data using the dataset with 'comp_labels' only\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39006bc4-8083-4f61-8e24-a1b4f8be1916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post11.tar.gz (3.6 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/job.9351895/pip-install-7799e_kd/sklearn/setup.py'\"'\"'; __file__='\"'\"'/tmp/job.9351895/pip-install-7799e_kd/sklearn/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/job.9351895/pip-install-7799e_kd/sklearn/pip-egg-info\n",
      "         cwd: /tmp/job.9351895/pip-install-7799e_kd/sklearn/\n",
      "    Complete output (18 lines):\n",
      "    The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "    rather than 'sklearn' for pip commands.\n",
      "    \n",
      "    Here is how to fix this error in the main use cases:\n",
      "    - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "    - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "      (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "    - if the 'sklearn' package is used by one of your dependencies,\n",
      "      it would be great if you take some time to track which package uses\n",
      "      'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "    - as a last resort, set the environment variable\n",
      "      SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "    \n",
      "    More information is available at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package\n",
      "    \n",
      "    If the previous advice does not cover your use case, feel free to report it at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f51b8261-739f-49d8-a457-2c02311591c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aedddd5b-7f91-484d-8a9a-a1080cc7d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy                         1.24.3    \n",
      "tensorflow                    2.13.1    \n",
      "tensorflow-estimator          2.13.0    \n",
      "tensorflow-io-gcs-filesystem  0.34.0    \n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/scratch/user/swati/pip_envs/my_notebook-Python/3.8.2/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep 'tensorflow\\|numpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d44af-1b22-4771-875b-b4db710d36d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
