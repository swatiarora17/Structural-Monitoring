{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b319da-a4d2-4ceb-90ed-66ad5192c2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 22:41:29.328604: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-28 22:41:29.940663: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-28 22:41:30.765219: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-28 22:41:30.877852: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 22:41:40.603345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2acaed3-0d87-4197-a0f1-6f665357e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shape_info(dir):\n",
    "    import pickle\n",
    "    with open(dir + '/pickleshapes', 'rb') as file:\n",
    "        # Load the data from the file\n",
    "        shapes = pickle.load(file)\n",
    "        return shapes\n",
    "def _lab_labels(all_labels):\n",
    "        #, this is an embedded function called from below\n",
    "        labels = {}\n",
    "        labels['comp_labels'] = tf.one_hot(tf.cast(all_labels[0],tf.int32),3)\n",
    "        labels['amp_labels'] = tf.one_hot(tf.cast(all_labels[1],tf.int32),10)\n",
    "        labels['torque_labels'] = tf.one_hot(tf.cast(all_labels[2],tf.int32),5)\n",
    "        labels['joint_labels'] = tf.one_hot(tf.cast(all_labels[3],tf.int32),3)\n",
    "        return labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa445be-82b6-40da-8159-7e4cbe8fe209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tf_data(folder,shape_info):\n",
    "    tf_records = glob.glob(os.path.join(folder, '*.tf'))\n",
    "    dataset = tf.data.TFRecordDataset(tf_records)\n",
    "    def _parse_function(example_proto):\n",
    "        feature_description = {\n",
    "            'ts': tf.io.FixedLenFeature(shape_info[0], tf.float32),\n",
    "            'labels': tf.io.FixedLenFeature(shape_info[1], tf.float32),\n",
    "            'pos_labels': tf.io.FixedLenFeature(shape_info[2], tf.float32)\n",
    "        }\n",
    "        #example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        example = tf.io.parse_example(example_proto, feature_description)\n",
    "        all_labels = tf.cast(example['labels'], tf.float32)\n",
    "        #all_labels = example['labels']\n",
    "        labels = _lab_labels(all_labels)\n",
    "        if 'pos_labels' in example.keys():\n",
    "            pos_labels = tf.cast(example['pos_labels'], tf.float32)\n",
    "            labels['pos_labels'] = pos_labels\n",
    "        ts = example['ts']\n",
    "        #labels = all_labels\n",
    "        return ts, labels['torque_labels']\n",
    "        #return example['ts'], labels\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    dataset = dataset.batch(batch_size=500, drop_remainder = False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8676d07f-20ce-46b9-8b6f-18be371473f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_base = '/scratch/user/swati/Capstone/'\n",
    "#dir_data = 'RoughCut_Datasets'\n",
    "dir_base = '/scratch/group/statconsult/Output_hema/'\n",
    "dir_data = 'Data6_Copy/'\n",
    "dir = dir_base + dir_data + '/'\n",
    "train_shape_info = load_shape_info(dir + 'train')\n",
    "test_shape_info = load_shape_info(dir + 'predict')\n",
    "predict_shape_info = load_shape_info(dir + 'validate')\n",
    "train_data = load_tf_data(dir + 'train',train_shape_info)\n",
    "test_data = load_tf_data(dir + 'predict',test_shape_info)\n",
    "predict_data = load_tf_data(dir + 'validate',predict_shape_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8e3b7e-39bf-470e-908c-85b26c1f33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_ts_batches(dataset):\n",
    "    combined_ts = []\n",
    "    train_labels = []\n",
    "    for ts, labels in dataset:\n",
    "        combined_ts.append(ts.numpy())\n",
    "        train_labels.append(labels.numpy())\n",
    "    combined_ts = np.concatenate(combined_ts, axis=0)\n",
    "    return combined_ts,train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8573ad5b-3f6a-4655-9774-3c8c9a44a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_top_features(pca_model, feature_names=None, top_n=5):\n",
    "    # Get the principal components and their weights\n",
    "    components = pca_model.components_\n",
    "\n",
    "    # If feature_names is not provided, create default feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature {i+1}\" for i in range(components.shape[1])]\n",
    "\n",
    "    # Plot bar charts for the top N features for each principal component\n",
    "    for i in range(components.shape[0]):\n",
    "        component_weights = list(zip(feature_names, components[i, :]))\n",
    "        component_weights.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        top_features = component_weights[:top_n]\n",
    "\n",
    "        features, weights = zip(*top_features)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(features, weights)\n",
    "        plt.title(f'Top {top_n} Features for Principal Component {i+1}')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Weight')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ab7e36-f2f4-40fd-ac6b-262dd93d92e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(dataset):\n",
    "    dataset_batch_size, sequence_length, feature_dim = dataset.shape\n",
    "    reshaped_data = tf.reshape(dataset, (dataset_batch_size * sequence_length, feature_dim))\n",
    "    print(dataset.shape)\n",
    "    batch_size = 500\n",
    "    dataset_batch_size = int(dataset_batch_size)\n",
    "    if dataset_batch_size < batch_size:\n",
    "        batch_size = dataset_batch_size   \n",
    "    pca = PCA(n_components=50)\n",
    "    pca_data = pca.fit_transform(reshaped_data)\n",
    "    #visualize_top_features(pca)\n",
    "    pca_result = tf.reshape(pca_data, (int(dataset_batch_size/batch_size),batch_size, sequence_length, 50))\n",
    "    return pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52a73fa-0dc8-4381-a341-210ef754e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reduced_dataset(ts_batches,label_batches):\n",
    "    ts_batches = tf.convert_to_tensor(ts_batches, dtype=tf.float64)\n",
    "    label_batches = tf.convert_to_tensor(label_batches, dtype=tf.float64)\n",
    "    \n",
    "    # Create a TensorFlow dataset\n",
    "    reduced_dataset = tf.data.Dataset.from_tensor_slices((ts_batches, label_batches))\n",
    "    return reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e68cc03-4499-4cb0-9e2c-0a6dfdc5258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_ts,train_labels = combine_ts_batches(train_data)\n",
    "combined_test_ts,test_labels = combine_ts_batches(test_data)\n",
    "combined_val_ts,val_labels = combine_ts_batches(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f6860ea-40ab-498e-8282-51297834fb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108000, 1000, 105)\n"
     ]
    }
   ],
   "source": [
    "pca_train_ts = apply_pca(combined_train_ts)\n",
    "train_reduced_dataset = generate_reduced_dataset(pca_train_ts,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc4414c-646e-43b7-9834-2372ce1a606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 1000, 105)\n"
     ]
    }
   ],
   "source": [
    "pca_test_ts = apply_pca(combined_test_ts)\n",
    "test_reduced_dataset = generate_reduced_dataset(pca_test_ts,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19f5ad93-4fa6-49aa-b075-5dff90818047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 1000, 105)\n"
     ]
    }
   ],
   "source": [
    "pca_val_ts = apply_pca(combined_val_ts)\n",
    "val_reduced_dataset = generate_reduced_dataset(pca_val_ts,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8153a569-b766-4327-8bae-0a1afd16b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 50000)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               6400128   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6408709 (24.45 MB)\n",
      "Trainable params: 6408709 (24.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# # Define the model\n",
    "# model = keras.Sequential()\n",
    "\n",
    "# # Flatten the input data\n",
    "# model.add(keras.layers.Flatten(input_shape=(1000, 50)))\n",
    "\n",
    "# # Add a couple of dense layers\n",
    "# model.add(keras.layers.Dense(128, activation='relu'))\n",
    "# model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "# # Output layer with 4 units (assuming you have 4 classes for classification)\n",
    "# model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Flatten(input_shape=(1000, 50)))\n",
    "# model.add(keras.layers.Dense(128, activation='relu'))\n",
    "# model.add(keras.layers.Dropout(0.5))  # Adding dropout for regularization\n",
    "# model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# model.add(keras.layers.Dropout(0.5))\n",
    "# model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# # Compile the model with a lower learning rate\n",
    "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "#model.summary()\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(1000, 50)))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))  # Adding dropout for regularization\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#     layers.Flatten(input_shape=(1000, 50)),\n",
    "#     layers.BatchNormalization(),\n",
    "    \n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "\n",
    "#     layers.Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Use a different optimizer (SGD) and add learning rate scheduling\n",
    "# opt = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c718f1a7-3f27-47f9-afc3-95c58d8ad2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0781 - accuracy: 0.9799 - val_loss: 2.7543 - val_accuracy: 0.4318\n",
      "Epoch 2/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0758 - accuracy: 0.9807 - val_loss: 2.7514 - val_accuracy: 0.4287\n",
      "Epoch 3/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0768 - accuracy: 0.9797 - val_loss: 2.7634 - val_accuracy: 0.4283\n",
      "Epoch 4/50\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.0727 - accuracy: 0.9806 - val_loss: 2.7768 - val_accuracy: 0.4293\n",
      "Epoch 5/50\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.0764 - accuracy: 0.9804 - val_loss: 2.7858 - val_accuracy: 0.4270\n",
      "Epoch 6/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0720 - accuracy: 0.9813 - val_loss: 2.7970 - val_accuracy: 0.4272\n",
      "Epoch 7/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0742 - accuracy: 0.9809 - val_loss: 2.7926 - val_accuracy: 0.4283\n",
      "Epoch 8/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0717 - accuracy: 0.9812 - val_loss: 2.8049 - val_accuracy: 0.4283\n",
      "Epoch 9/50\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.0723 - accuracy: 0.9814 - val_loss: 2.8146 - val_accuracy: 0.4293\n",
      "Epoch 10/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0746 - accuracy: 0.9807 - val_loss: 2.7975 - val_accuracy: 0.4275\n",
      "Epoch 11/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0741 - accuracy: 0.9806 - val_loss: 2.7750 - val_accuracy: 0.4310\n",
      "Epoch 12/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0719 - accuracy: 0.9817 - val_loss: 2.7876 - val_accuracy: 0.4333\n",
      "Epoch 13/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0692 - accuracy: 0.9824 - val_loss: 2.8094 - val_accuracy: 0.4313\n",
      "Epoch 14/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0676 - accuracy: 0.9826 - val_loss: 2.8472 - val_accuracy: 0.4330\n",
      "Epoch 15/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0705 - accuracy: 0.9818 - val_loss: 2.8334 - val_accuracy: 0.4317\n",
      "Epoch 16/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0704 - accuracy: 0.9820 - val_loss: 2.8250 - val_accuracy: 0.4292\n",
      "Epoch 17/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0723 - accuracy: 0.9822 - val_loss: 2.8022 - val_accuracy: 0.4285\n",
      "Epoch 18/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0689 - accuracy: 0.9827 - val_loss: 2.8102 - val_accuracy: 0.4312\n",
      "Epoch 19/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0672 - accuracy: 0.9831 - val_loss: 2.8703 - val_accuracy: 0.4328\n",
      "Epoch 20/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0722 - accuracy: 0.9813 - val_loss: 2.8133 - val_accuracy: 0.4362\n",
      "Epoch 21/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0681 - accuracy: 0.9829 - val_loss: 2.8086 - val_accuracy: 0.4357\n",
      "Epoch 22/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0672 - accuracy: 0.9826 - val_loss: 2.8411 - val_accuracy: 0.4388\n",
      "Epoch 23/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0652 - accuracy: 0.9830 - val_loss: 2.8854 - val_accuracy: 0.4385\n",
      "Epoch 24/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0672 - accuracy: 0.9828 - val_loss: 2.8921 - val_accuracy: 0.4363\n",
      "Epoch 25/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0656 - accuracy: 0.9839 - val_loss: 2.8943 - val_accuracy: 0.4373\n",
      "Epoch 26/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0645 - accuracy: 0.9838 - val_loss: 2.9280 - val_accuracy: 0.4353\n",
      "Epoch 27/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0666 - accuracy: 0.9828 - val_loss: 2.9358 - val_accuracy: 0.4347\n",
      "Epoch 28/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0665 - accuracy: 0.9829 - val_loss: 2.8934 - val_accuracy: 0.4325\n",
      "Epoch 29/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0646 - accuracy: 0.9835 - val_loss: 2.9373 - val_accuracy: 0.4337\n",
      "Epoch 30/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0616 - accuracy: 0.9840 - val_loss: 2.9499 - val_accuracy: 0.4335\n",
      "Epoch 31/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0622 - accuracy: 0.9838 - val_loss: 2.9675 - val_accuracy: 0.4365\n",
      "Epoch 32/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0633 - accuracy: 0.9844 - val_loss: 2.9810 - val_accuracy: 0.4308\n",
      "Epoch 33/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0627 - accuracy: 0.9840 - val_loss: 2.9875 - val_accuracy: 0.4307\n",
      "Epoch 34/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0620 - accuracy: 0.9844 - val_loss: 2.9989 - val_accuracy: 0.4332\n",
      "Epoch 35/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0638 - accuracy: 0.9837 - val_loss: 2.9821 - val_accuracy: 0.4350\n",
      "Epoch 36/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0609 - accuracy: 0.9850 - val_loss: 2.9987 - val_accuracy: 0.4343\n",
      "Epoch 37/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0600 - accuracy: 0.9845 - val_loss: 3.0273 - val_accuracy: 0.4302\n",
      "Epoch 38/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0591 - accuracy: 0.9847 - val_loss: 3.0310 - val_accuracy: 0.4320\n",
      "Epoch 39/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0591 - accuracy: 0.9845 - val_loss: 3.0607 - val_accuracy: 0.4303\n",
      "Epoch 40/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0580 - accuracy: 0.9853 - val_loss: 3.0434 - val_accuracy: 0.4328\n",
      "Epoch 41/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0571 - accuracy: 0.9849 - val_loss: 3.0904 - val_accuracy: 0.4333\n",
      "Epoch 42/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0594 - accuracy: 0.9848 - val_loss: 3.0371 - val_accuracy: 0.4335\n",
      "Epoch 43/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0594 - accuracy: 0.9848 - val_loss: 3.0649 - val_accuracy: 0.4305\n",
      "Epoch 44/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0595 - accuracy: 0.9847 - val_loss: 3.0378 - val_accuracy: 0.4322\n",
      "Epoch 45/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0552 - accuracy: 0.9852 - val_loss: 3.0787 - val_accuracy: 0.4332\n",
      "Epoch 46/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0557 - accuracy: 0.9855 - val_loss: 3.1362 - val_accuracy: 0.4310\n",
      "Epoch 47/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0546 - accuracy: 0.9861 - val_loss: 3.1740 - val_accuracy: 0.4323\n",
      "Epoch 48/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0601 - accuracy: 0.9845 - val_loss: 3.0963 - val_accuracy: 0.4287\n",
      "Epoch 49/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0560 - accuracy: 0.9860 - val_loss: 3.1137 - val_accuracy: 0.4323\n",
      "Epoch 50/50\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.0575 - accuracy: 0.9855 - val_loss: 3.1098 - val_accuracy: 0.4305\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Train the model\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)\n",
    "\n",
    "#history = model1.fit(train_reduced_dataset, callbacks=[lr_scheduler],epochs=num_epochs, validation_data=val_reduced_dataset)\n",
    "history = model.fit(train_reduced_dataset,epochs=num_epochs, validation_data=val_reduced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34331733-29ce-47f4-ba28-9a6e1294e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 12ms/step - loss: 2.1326 - accuracy: 0.5203\n",
      "Test Accuracy: 52.03%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on your test data using the dataset with 'comp_labels' only\n",
    "test_loss, test_accuracy = model.evaluate(test_reduced_dataset)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39006bc4-8083-4f61-8e24-a1b4f8be1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b8261-739f-49d8-a457-2c02311591c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedddd5b-7f91-484d-8a9a-a1080cc7d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep 'tensorflow\\|numpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d44af-1b22-4771-875b-b4db710d36d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
